{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrYle7CYXSH7"
      },
      "source": [
        "***Transfer learningâ€‘based CNN for plant leaf\n",
        "disease detection***\n",
        "\n",
        " **CVPR[B] Group-10**\n",
        "* MD. ASIFUR RAHMAN - - -[20-43064-1]\n",
        "* ABRAR IBNE AHSAN - - - [19-41389-3]\n",
        "* SHINZON SIDDIQUA - - - [20-43671-2]\n",
        "* H.M.Saifullahil Mazid- [20-44018-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3WB7KdmRwep"
      },
      "outputs": [],
      "source": [
        "# CVPR[B] Group-10\n",
        "#MD. ASIFUR RAHMAN - - - [20-43064-1]\n",
        "#ABRAR IBNE AHSAN -\t - - [19-41389-3]\n",
        "#SHINZON SIDDIQUA - - - -[20-43671-2]\n",
        "#H.M.Saifullahil Mazid- -[20-44018-2]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import layers, Sequential, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdVThL6WR2gS"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "#source and destination directories\n",
        "source_directory = \"/kaggle/input/plant-village/PlantVillage\"\n",
        "destination_directory = \"/kaggle/output/PlantVillage\"\n",
        "\n",
        "folders_to_copy = ['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']\n",
        "\n",
        "#if it doesn't exist\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "for folder_name in folders_to_copy:\n",
        "    source_folder = os.path.join(source_directory, folder_name)\n",
        "    destination_folder = os.path.join(destination_directory, folder_name)\n",
        "\n",
        "    try:\n",
        "        shutil.copytree(source_folder, destination_folder)\n",
        "        print(f\"Folder '{folder_name}' copied successfully.\")\n",
        "    except FileExistsError:\n",
        "        # if folder already exists\n",
        "        shutil.copy2(source_folder, destination_folder)\n",
        "        print(f\"Folder '{folder_name}' contents copied successfully.\")\n",
        "    except shutil.Error as e:\n",
        "        print(f\"Error copying folder '{folder_name}': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfPRSNdhVITw"
      },
      "outputs": [],
      "source": [
        "# Initializing some Constants\n",
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 32\n",
        "CHANNELS = 3\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jR7hMf-FVO3Z"
      },
      "outputs": [],
      "source": [
        "# Creating the image dataset\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/kaggle/output/PlantVillage\",\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        ")\n",
        "# class names\n",
        "class_names = dataset.class_names\n",
        "print(\"Class Names:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9lq2T2TVRZP"
      },
      "outputs": [],
      "source": [
        "for image_batch, labels_batch in dataset.take(1):\n",
        "    print(image_batch.shape)\n",
        "    print(labels_batch.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocFsA8ZjVT-W"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4jWMSsFVWPZ"
      },
      "outputs": [],
      "source": [
        "# Data Visualization\n",
        "plt.figure(figsize=(12,8))\n",
        "for image_batch, label_batch in dataset.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(3, 4,i+1)\n",
        "        plt.imshow(image_batch[i].numpy().astype('uint8'))\n",
        "        plt.title(class_names[label_batch[i].numpy()], fontsize = 7)\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPajiGeOVYNu"
      },
      "outputs": [],
      "source": [
        "def get_dataset_partitions(dataset, train_size=0.8, val_size=0.1, test_size=0.1, shuffle=True, shuffle_size=10000):\n",
        "    # Length of Dataset\n",
        "    dataset_size = len(dataset)\n",
        "\n",
        "    # Shuffling\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(shuffle_size, seed=12)\n",
        "\n",
        "    # Splitting\n",
        "    train_size = int(dataset_size * train_size)\n",
        "    val_size = int(dataset_size * val_size)\n",
        "\n",
        "    # Training\n",
        "    train_dataset = dataset.take(train_size)\n",
        "\n",
        "    # Validation and Testing\n",
        "    val_dataset = dataset.skip(train_size).take(val_size)\n",
        "    test_dataset = dataset.skip(train_size).skip(val_size)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtRUM9c3Vaea"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = get_dataset_partitions(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um_k4USFVdRX"
      },
      "outputs": [],
      "source": [
        "print(f\"Length of Training Dataset: {len(train_dataset)}\")\n",
        "print(f\"Length of Validation Dataset: {len(val_dataset)}\")\n",
        "print(f\"Length of Testing Dataset: {len(test_dataset)}\")\n",
        "print(f\"Is the length of the whole dataset equal to the length of the sum of the splits?\",\n",
        "     (len(dataset) == (len(train_dataset) + len(val_dataset) + len(test_dataset))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQK-hrJ8VfTV"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3dYBYF9Vg6b"
      },
      "outputs": [],
      "source": [
        "# Initializing some constants\n",
        "N_CLASSES = len(class_names)\n",
        "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyn7ahsfVizO"
      },
      "outputs": [],
      "source": [
        "resize_rescale = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.Resizing(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    layers.experimental.preprocessing.Rescaling(1.,255)\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\n",
        "    layers.experimental.preprocessing.RandomRotation(0.2)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iZIKohnVkls"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(resize_rescale)\n",
        "model.add(data_augmentation)\n",
        "model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(N_CLASSES, activation='softmax'))\n",
        "\n",
        "model.build(input_shape=input_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ymtF56OVm06"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxtTZD-oVpPi"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_data=val_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNBvM0zSVr0Y"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFsK8kuoVvzk"
      },
      "outputs": [],
      "source": [
        "history.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HsvDpRHVxdF"
      },
      "outputs": [],
      "source": [
        "history.history.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz36VYPMVzg3"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T23OUR9WV1PN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
        "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
        "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Nxj4kTV28n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for images_batch, labels_batch in test_dataset.take(1):\n",
        "\n",
        "    first_image = images_batch[0].numpy().astype('uint8')\n",
        "    first_label = labels_batch[0].numpy()\n",
        "\n",
        "    print(\"first image to predict\")\n",
        "    plt.imshow(first_image)\n",
        "    print(\"actual label:\",class_names[first_label])\n",
        "\n",
        "    batch_prediction = model.predict(images_batch)\n",
        "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaDT4j3qV5Q1"
      },
      "outputs": [],
      "source": [
        "def predict(model, img):\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    predicted_class = class_names[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
        "    return predicted_class, confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U24mLtMsV7TB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in test_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "\n",
        "        predicted_class, confidence = predict(model, images[i].numpy())\n",
        "        actual_class = class_names[labels[i]]\n",
        "\n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
        "\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D49jbc4SXJQw"
      },
      "outputs": [],
      "source": [
        "# CVPR[B] Group-10\n",
        "#MD. ASIFUR RAHMAN - - - [20-43064-1]\n",
        "#ABRAR IBNE AHSAN -\t - - [19-41389-3]\n",
        "#SHINZON SIDDIQUA - - - -[20-43671-2]\n",
        "#H.M.Saifullahil Mazid- -[20-44018-2]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
